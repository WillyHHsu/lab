{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from scores.score_logger import ScoreLogger\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seu jogo de escolha\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 1000\n",
    "BATCH_SIZE = 20\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.05\n",
    "EXPLORATION_DECAY = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def cartpole\n",
    "env = gym.make(ENV_NAME)\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        \n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen = MEMORY_SIZE)        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_stage, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand()< exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def exploration_rate(self):\n",
    "        if len(self.memory)<BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update =  (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose = 0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0') # try for different environements\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03570481,  0.01183121,  0.0260376 ,  0.03959878])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://gist.github.com/iambrian/2bcc8fc03eaecb2cbe53012d2f505465\n",
    "# import gym\n",
    "# env = gym.make('CartPole-v0')\n",
    "# highscore = 0\n",
    "# for i_episode in range(20): # run 20 episodes\n",
    "#     observation = env.reset()\n",
    "#     points = 0 # keep track of the reward each episode\n",
    "#     while True: # run until episode is done\n",
    "#         env.render()\n",
    "#         action = 1 if observation[2] > 0 else 0 # if angle if positive, move right. if angle is negative, move left\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         points += reward\n",
    "#         if done:\n",
    "#           if points > highscore: # record high score\n",
    "#             highscore = points\n",
    "#             break\n",
    "            \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset() # initial observation\n",
    "    aux = 0\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        aux += 1\n",
    "        print('{}.observation: {}'.format(aux, observation))\n",
    "        action = env.action_space.sample()\n",
    "        print('--ação:{}'.format(action))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #ok onde está as condições de done?\n",
    "        print('--observation:{}, reward:{}, done:{}, info:{}'.format(observation, reward, done, info))\n",
    "        if done:\n",
    "            print(\"\\n\")\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1).upper())\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -9.13693864, -12.93749645,   4.91046668,  -2.18595879]),\n",
       " 0.0,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space, action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps = 150\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.95\n",
    "\n",
    "exploration_rate = 0.98\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "1000 :  0.04900000000000004\n",
      "2000 :  0.19700000000000015\n",
      "3000 :  0.3900000000000003\n",
      "4000 :  0.5210000000000004\n",
      "5000 :  0.5810000000000004\n",
      "6000 :  0.5890000000000004\n",
      "7000 :  0.6180000000000004\n",
      "8000 :  0.6560000000000005\n",
      "9000 :  0.5910000000000004\n",
      "10000 :  0.5760000000000004\n",
      "--------------------\n",
      "[[0.18506404 0.15213222 0.13944287 0.15344793]\n",
      " [0.09012277 0.10476312 0.0880154  0.15664491]\n",
      " [0.16215522 0.09718711 0.09776034 0.10215043]\n",
      " [0.08761844 0.08485919 0.08073325 0.09676052]\n",
      " [0.21098478 0.14862339 0.08831195 0.12975429]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06879404 0.05552519 0.15762491 0.05245866]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11445532 0.21481454 0.20107123 0.25525668]\n",
      " [0.28174888 0.31323646 0.25846396 0.2022548 ]\n",
      " [0.37004345 0.22243588 0.14910161 0.1476606 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32441187 0.30260033 0.43181019 0.30236237]\n",
      " [0.50391656 0.71419107 0.49910086 0.56423663]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "reward_episodes_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #env.render()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) +\\\n",
    "        learning_rate*(reward + discount_rate*np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)* \\\n",
    "        np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    reward_episodes_list.append(rewards_current_episode)\n",
    "\n",
    "reward_per_k = np.split(np.array(reward_episodes_list), num_episodes/1000)\n",
    "count = 1000\n",
    "print('--'*10)\n",
    "for r in reward_per_k:\n",
    "    print(count, ': ', str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "print('--'*10)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
