{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2b1472d67bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultioutput\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiOutputRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScoreLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scores'"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from scores.score_logger import ScoreLogger\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seu jogo de escolha\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 1000\n",
    "BATCH_SIZE = 20\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.05\n",
    "EXPLORATION_DECAY = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def cartpole\n",
    "env = gym.make(ENV_NAME)\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        \n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen = MEMORY_SIZE)        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_stage, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand()< exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def exploration_rate(self):\n",
    "        if len(self.memory)<BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update =  (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose = 0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0') # try for different environements\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03570481,  0.01183121,  0.0260376 ,  0.03959878])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://gist.github.com/iambrian/2bcc8fc03eaecb2cbe53012d2f505465\n",
    "# import gym\n",
    "# env = gym.make('CartPole-v0')\n",
    "# highscore = 0\n",
    "# for i_episode in range(20): # run 20 episodes\n",
    "#     observation = env.reset()\n",
    "#     points = 0 # keep track of the reward each episode\n",
    "#     while True: # run until episode is done\n",
    "#         env.render()\n",
    "#         action = 1 if observation[2] > 0 else 0 # if angle if positive, move right. if angle is negative, move left\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         points += reward\n",
    "#         if done:\n",
    "#           if points > highscore: # record high score\n",
    "#             highscore = points\n",
    "#             break\n",
    "            \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset() # initial observation\n",
    "    aux = 0\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        aux += 1\n",
    "        print('{}.observation: {}'.format(aux, observation))\n",
    "        action = env.action_space.sample()\n",
    "        print('--ação:{}'.format(action))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #ok onde está as condições de done?\n",
    "        print('--observation:{}, reward:{}, done:{}, info:{}'.format(observation, reward, done, info))\n",
    "        if done:\n",
    "            print(\"\\n\")\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1).upper())\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -9.13693864, -12.93749645,   4.91046668,  -2.18595879]),\n",
       " 0.0,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space, action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps = 150\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.95\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "1000 :  0.048000000000000036\n",
      "2000 :  0.18600000000000014\n",
      "3000 :  0.34800000000000025\n",
      "4000 :  0.4850000000000004\n",
      "5000 :  0.5940000000000004\n",
      "6000 :  0.6500000000000005\n",
      "7000 :  0.5970000000000004\n",
      "8000 :  0.6100000000000004\n",
      "9000 :  0.6420000000000005\n",
      "10000 :  0.6830000000000005\n",
      "--------------------\n",
      "[[0.15395433 0.15133235 0.16772163 0.15368095]\n",
      " [0.09034392 0.08680864 0.05423558 0.16323595]\n",
      " [0.18362902 0.10506937 0.11097827 0.09712907]\n",
      " [0.07692466 0.07705253 0.05608361 0.09707123]\n",
      " [0.20396458 0.13590793 0.13583458 0.16676089]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06752913 0.09046227 0.2698702  0.03622247]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.15837212 0.17502338 0.15160105 0.2415143 ]\n",
      " [0.31607278 0.35896916 0.25476007 0.20933769]\n",
      " [0.45711262 0.31191563 0.30606706 0.26553679]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30892439 0.44177871 0.5213282  0.27047843]\n",
      " [0.52930222 0.76875506 0.62971935 0.66591678]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "reward_episodes_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #env.render()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) +\\\n",
    "        learning_rate*(reward + discount_rate*np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)* \\\n",
    "        np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    reward_episodes_list.append(rewards_current_episode)\n",
    "\n",
    "reward_per_k = np.split(np.array(reward_episodes_list), num_episodes/1000)\n",
    "count = 1000\n",
    "print('--'*10)\n",
    "for r in reward_per_k:\n",
    "    print(count, ': ', str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "print('--'*10)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.observation_space.n\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6614\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "counter = 0\n",
    "reward = None\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 Total Reward: -117\n",
      "Episode 100 Total Reward: 8\n",
      "Episode 150 Total Reward: -76\n",
      "Episode 200 Total Reward: -20\n",
      "Episode 250 Total Reward: -1\n",
      "Episode 300 Total Reward: 9\n",
      "Episode 350 Total Reward: 11\n",
      "Episode 400 Total Reward: -7\n",
      "Episode 450 Total Reward: 10\n",
      "Episode 500 Total Reward: 10\n",
      "Episode 550 Total Reward: 2\n",
      "Episode 600 Total Reward: 5\n",
      "Episode 650 Total Reward: 6\n",
      "Episode 700 Total Reward: 8\n",
      "Episode 750 Total Reward: 10\n",
      "Episode 800 Total Reward: 6\n",
      "Episode 850 Total Reward: 7\n",
      "Episode 900 Total Reward: 11\n",
      "Episode 950 Total Reward: 9\n",
      "Episode 1000 Total Reward: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "            action = np.argmax(Q[state]) #1\n",
    "            state2, reward, done, info = env.step(action) #2\n",
    "            Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n",
    "            G += reward\n",
    "            state = state2   \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "state = env.reset()\n",
    "env.render()\n",
    "env.action_space.n\n",
    "env.env.get_action_meanings()\n",
    "state = env.reset()\n",
    "reward, info, done = None, None, None\n",
    "while done != True:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PI = 3.1415926\n",
    "e = 2.71828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_number(min_value, max_value):\n",
    "    \"\"\"\n",
    "    This function gets a random number from a uniform distribution between\n",
    "    the two input values [min_value, max_value] inclusively\n",
    "    Args:\n",
    "    - min_value (float)\n",
    "    - max_value (float)\n",
    "    Return:\n",
    "    - Random number between this range (float)\n",
    "    \"\"\"\n",
    "    range = max_value - min_value\n",
    "    choice = random.uniform(0,1)\n",
    "    return min_value + range*choice\n",
    "def f_of_x(x):\n",
    "    \"\"\"\n",
    "    This is the main function we want to integrate over.\n",
    "    Args:\n",
    "    - x (float) : input to function; must be in radians\n",
    "    Return:\n",
    "    - output of function f(x) (float)\n",
    "    \"\"\"\n",
    "    return (e**(-1*x))/(1+(x-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crude_monte_carlo(num_samples=5000):\n",
    "    \"\"\"\n",
    "    This function performs the Crude Monte Carlo for our\n",
    "    specific function f(x) on the range x=0 to x=5.\n",
    "    Notice that this bound is sufficient because f(x)\n",
    "    approaches 0 at around PI.\n",
    "    Args:\n",
    "    - num_samples (float) : number of samples\n",
    "    Return:\n",
    "    - Crude Monte Carlo estimation (float)\n",
    "    \n",
    "    \"\"\"\n",
    "    lower_bound = 0\n",
    "    upper_bound = 5\n",
    "    \n",
    "    sum_of_samples = 0\n",
    "    for i in range(num_samples):\n",
    "        x = get_rand_number(lower_bound, upper_bound)\n",
    "        sum_of_samples += f_of_x(x)\n",
    "    \n",
    "    return (upper_bound - lower_bound) * float(sum_of_samples/num_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
