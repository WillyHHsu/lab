{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndocker start spark\\n\\n127.0.0.1:8888/?token=c931dec8954620e490c7fa48529b766a83e7dcb47e9ec19a\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "'''\n",
    "docker start spark\n",
    "\n",
    "127.0.0.1:8888/?token=c931dec8954620e490c7fa48529b766a83e7dcb47e9ec19a\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux 890d146aa60d 4.15.0-46-generic #49-Ubuntu SMP Wed Feb 6 09:33:07 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('teste').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/train.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = ['Sex']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, \n",
    "                                  outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], \n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['Age']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(stages = stages)\n",
    "# pipelineModel = pipeline.fit(df)\n",
    "# df = pipelineModel.transform(df)\n",
    "# selectedCols = ['label', 'features'] + cols\n",
    "# df = df.select(selectedCols)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where('number % 2 = 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = spark.read.option('interSchema', 'true').option('header', 'true').csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [Sex#170 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Sex#170 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [PassengerId#166,Survived#167,Pclass#168,Name#169,Sex#170,Age#171,SibSp#172,Parch#173,Ticket#174,Fare#175,Cabin#176,Embarked#177] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/git/personal/lab/spark101/data/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PassengerId:string,Survived:string,Pclass:string,Name:string,Sex:string,Age:string,SibSp:s...\n"
     ]
    }
   ],
   "source": [
    "titanic.sort('Sex').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL mode\n",
    "titanic.createOrReplaceTempView('titanic')\n",
    "sqlWay = spark.sql('Select Cabin from titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Cabin: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlWay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "a  = titanic.groupBy('Sex').count().withColumnRenamed('count(\"Sex\")', 'soma').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Sex='female', count=314), Row(Sex='male', count=577)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015\\\n",
    ".groupBy(“DEST_COUNTRY_NAME”)\\\n",
    ".sum(“count”)\\\n",
    ".withColumnRenamed(“sum(count)”, “destination_total”)\\\n",
    ".sort(desc(“destination_total”))\\\n",
    ".limit(5)\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format('csv').option('header', 'true')\\\n",
    ".option('inferSchema', 'true').load('data/train.csv')\n",
    "\n",
    "staticDataFrame.createOrReplaceTempView('dead_data')\n",
    "staticSchema = staticDataFrame.schema\n",
    "\n",
    "streamingDataFrame = spark.readStream.schema(staticSchema)\\\n",
    ".option('maxFilesPerTrigger', 1).format('csv')\\\n",
    ".option('header', 'true').load('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
